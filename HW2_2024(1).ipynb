{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0d66b6",
   "metadata": {},
   "source": [
    "(HW2_2024)=\n",
    "# HW2\n",
    "\n",
    "**Deadline:** 03.11.2024 23:59 (GMT+5).\n",
    "\n",
    "In this task you are suggested to implement gradient descent and Newton's optimization methods, and investigate their performance for three types of functions:\n",
    "\n",
    "* quadratic function\n",
    "* loss function of linear regression\n",
    "* loss function of logistic regression\n",
    "\n",
    "`BaseSmoothOracle` is an abstract class for objective function $f$ (**oracle**). Each concrete oracle must inherit this class, implementing four methods:\n",
    "\n",
    "* `func(x)` calculates $f(\\boldsymbol x)$\n",
    "* `grad(x)` calculates $\\nabla f(\\boldsymbol x)$\n",
    "* `hess(x)` calculates $\\nabla^2 f(\\boldsymbol x)$\n",
    "* `get_opt(x)` calculates point of minimum by direct formula (works only for quadratic and linear regression oracles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe4a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSmoothOracle(object):\n",
    "    \"\"\"\n",
    "    Base class for implementation of oracles.\n",
    "    \"\"\"\n",
    "    def func(self, x):\n",
    "        \"\"\"\n",
    "        Computes the value of function at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"\n",
    "        Computes the gradient at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "    \n",
    "    def hess(self, x):\n",
    "        \"\"\"\n",
    "        Computes the Hessian matrix at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Hessian oracle is not implemented.')\n",
    "        \n",
    "    def get_opt(self):\n",
    "        \"\"\"\n",
    "        Computes the point x at which minimum is attained\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('True solution is unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec57e0",
   "metadata": {},
   "source": [
    "## Quadratic oracle\n",
    "\n",
    "### Task 2.1 (1 point)\n",
    "\n",
    "Implement quadratic oracle\n",
    "\n",
    "```{math}\n",
    ":label: quadratic-oracle\n",
    "    f(\\boldsymbol x) = \\frac 12 \\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax} - \\boldsymbol b^\\mathsf{T} \\boldsymbol x, \\quad \\boldsymbol A^\\mathsf{T}  = \\boldsymbol A \\in \\mathbb R^{n\\times n}, \\quad \\boldsymbol b\\in\\mathbb R^n\n",
    "```\n",
    "\n",
    "<!-- Let $\\boldsymbol A$ be a symmetric positive definite matrix. Solve the optimization task\n",
    "\n",
    "$$\n",
    "    f(\\boldsymbol x) = \\frac 12 \\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax} - \\boldsymbol b^\\mathsf{T}\\boldsymbol x \\to \\min\\limits_{\\boldsymbol x \\in \\mathbb R^n}\n",
    "$$\n",
    "\n",
    "analytically. Namely, -->\n",
    "\n",
    "Namely,\n",
    "\n",
    "* find solution $\\boldsymbol x_*$ of the equation $\\nabla f(\\boldsymbol x) = \\boldsymbol 0$ \n",
    "* find hessian $\\nabla^2 f$ and show that $\\nabla^2 f(\\boldsymbol x_*)$ is positive definite\n",
    "* prove that $\\boldsymbol x_*$ is the point of global minimum of $f$\n",
    "* finally, fill the gaps in cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4986f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QuadraticOracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for quadratic function:\n",
    "       func(x) = 1/2 x^TAx - b^Tx.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, b):\n",
    "        if not np.allclose(A, A.T):\n",
    "            raise ValueError('A should be a symmetric matrix.')\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "\n",
    "    def func(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def grad(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def hess(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def get_opt(self):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df142c",
   "metadata": {},
   "source": [
    "Plot levels of the quadraric oracle in 2-dimensional case using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc39e08b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def plot_levels(func, xrange=None, yrange=None, levels=None):\n",
    "    \"\"\"\n",
    "    Plotting the contour lines of the function.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.array([[1.0, 2.0], [2.0, 5.0]]), np.zeros(2))\n",
    "    >> plot_levels(oracle.func)\n",
    "    \"\"\"\n",
    "    if xrange is None:\n",
    "        xrange = [-6, 6]\n",
    "    if yrange is None:\n",
    "        yrange = [-5, 5]\n",
    "    if levels is None:\n",
    "        levels = [0, 0.25, 1, 4, 9, 16, 25]\n",
    "        \n",
    "    x = np.linspace(xrange[0], xrange[1], 100)\n",
    "    y = np.linspace(yrange[0], yrange[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            Z[i, j] = func(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "    CS = plt.contour(X, Y, Z, levels=levels, colors='k')\n",
    "    plt.clabel(CS, inline=1, fontsize=8) \n",
    "    plt.grid()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036fa89",
   "metadata": {},
   "source": [
    "## Linear regression oracle\n",
    "\n",
    "### Task 2.2 (1.5 points)\n",
    "\n",
    "Implement linear regression oracle\n",
    "\n",
    "$$\n",
    "    f(\\boldsymbol x) = \\frac 1m \\Vert \\boldsymbol{Ax} - \\boldsymbol b\\Vert_2^2, \\quad \\boldsymbol A \\in \\mathbb R^{m\\times n}, \\quad \\boldsymbol b \\in \\mathbb R^n.\n",
    "$$\n",
    "\n",
    "Namely,\n",
    "\n",
    "* find $\\nabla f(\\boldsymbol x)$\n",
    "* find $\\nabla^2 f(\\boldsymbol x)$\n",
    "* find analytic solution of the minimization problem $f(\\boldsymbol x) \\to \\min\\limits_{\\boldsymbol x}$\n",
    "* finally, fill the gaps in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db56aa11",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "class LinRegOracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for linear regression:\n",
    "       func(x) = 1/m ||Ax - b||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, b):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "\n",
    "    def func(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def grad(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def hess(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def get_opt(self):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bf248",
   "metadata": {},
   "source": [
    "## Logistic regression oracle\n",
    "\n",
    "### Task 2.3 (1.5 point)\n",
    "\n",
    "Implement logistic regression oracle\n",
    "\n",
    "```{math}\n",
    ":label: log-reg-oracle\n",
    "    f(\\boldsymbol x) = \\frac 1m \\sum_{i=1}^m \\log\\big(1 + \\exp(-b_i\\boldsymbol a_i^\\mathsf{T} \\boldsymbol x)\\big) + \\frac C2\\Vert \\boldsymbol x\\Vert_2^2\n",
    "```\n",
    "\n",
    "Namely, denote\n",
    "\n",
    "$$\n",
    "    \\boldsymbol A = \\begin{pmatrix} a_1^\\mathsf{T} \\\\\n",
    "    a_2^\\mathsf{T} \\\\\n",
    "    \\vdots\\\\\n",
    "    a_m^\\mathsf{T}\n",
    "    \\end{pmatrix} \\in \\mathbb R^{m\\times n}, \\quad\n",
    "    \\boldsymbol b = \\begin{pmatrix} b_1 \\\\\n",
    "    b_2\\\\\n",
    "    \\vdots\\\\\n",
    "    b_m\n",
    "    \\end{pmatrix}\\in \\mathbb R^m,\n",
    "    \\quad\n",
    "    \\boldsymbol x = \\begin{pmatrix} x_1 \\\\\n",
    "    x_2\\\\\n",
    "    \\vdots\\\\\n",
    "    x_n\n",
    "    \\end{pmatrix}\\in \\mathbb R^n,\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "* find the gradient $\\nabla f(\\boldsymbol x)$ and hessian $\\nabla^2 f(\\boldsymbol x)$ of logistic regression oracle {eq}`log-reg-oracle` in matrix-vector form\n",
    "* fill the gaps in the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1e7293",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit\n",
    "\n",
    "class LogRegOracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, b, regcoef=1):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.C = regcoef\n",
    "\n",
    "    def func(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def grad(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def hess(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass \n",
    "    \n",
    "    def get_opt(self):\n",
    "        \"\"\"\n",
    "        Since there is no analytic solution for the optimal weights of logistic regression, \n",
    "        here we have to cheat a bit\n",
    "        \"\"\"\n",
    "        LR = LogisticRegression(fit_intercept=False, C= 1./self.C/self.A.shape[0])\n",
    "        LR.fit(self.A, (self.b + 1) / 2)\n",
    "        return LR.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd2b22",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Given a starting point $\\boldsymbol x_0 \\in\\mathbb R^n$ and learning rate $\\eta > 0$, iteratively calculate\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{k+1} = \\boldsymbol x_k - \\eta \\nabla f(\\boldsymbol x_k), \\quad k = 0, 1,\\ldots\n",
    "$$\n",
    "\n",
    "The process should stop when either $k = \\mathrm{max\\_iter}$ or \n",
    "\n",
    "$$\n",
    "\\Vert \\nabla f(\\boldsymbol x_k)\\Vert_2^2 \\leqslant \\varepsilon \\Vert \\nabla f(\\boldsymbol x_0)\\Vert_2^2\n",
    "$$\n",
    "\n",
    "where $\\varepsilon > 0$ is tolerance.\n",
    "\n",
    "### Task 2.4 (1 point)\n",
    "\n",
    "Complete the implementation of gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6794e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def gradient_descent(oracle, x_0, learning_rate, tolerance=1e-5, max_iter=10000,\n",
    "                     trace=False, display=False):\n",
    "    \"\"\"\n",
    "    Gradient descent optimization method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oracle : BaseSmoothOracle-descendant object\n",
    "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
    "        function value, its gradient and Hessian respectively.\n",
    "    x_0 : np.array\n",
    "        Starting point for optimization algorithm.\n",
    "    learning_rate : float\n",
    "        Learning rate.\n",
    "    tolerance : float\n",
    "        Epsilon value for stopping criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    trace : bool\n",
    "        If True, the progress information is appended into history dictionary during training.\n",
    "        Otherwise None is returned instead of history.\n",
    "    display : bool\n",
    "        If True, debug information is displayed during optimization.\n",
    "        Printing format and is up to a student and is not checked in any way.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_star : np.array\n",
    "        The point found by the optimization procedure\n",
    "    message : string\n",
    "        \"success\" or the description of error:\n",
    "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
    "                the stopping criterion.\n",
    "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
    "    history : dictionary of lists or None\n",
    "        Dictionary containing the progress information or None if trace=False.\n",
    "        Dictionary has to be organized as follows:\n",
    "            - history['time'] : list of floats, containing time in seconds passed from the start of the method\n",
    "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
    "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
    "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
    "    >> x_opt, message, history = gradient_descent(oracle, np.zeros(5))\n",
    "    >> print('Found optimal point: {}'.format(x_opt))\n",
    "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
    "    \"\"\"\n",
    "    history = defaultdict(list) if trace else None\n",
    "    x_k = np.copy(x_0)\n",
    "    grad_at_x_0 = oracle.grad(x_0)\n",
    "\n",
    "    INF = 1e100\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for i in range(max_iter + 1):     \n",
    "        grad_at_x_k = oracle.grad(x_k)\n",
    "        if np.any(np.abs(x_k) > INF) or np.any(np.abs(grad_at_x_k) > INF):\n",
    "            return x_k, 'computational_error', history\n",
    "\n",
    "        if trace:\n",
    "            history['time'].append((datetime.now() - start_time).microseconds * 1e-6)\n",
    "            history['func'].append(oracle.func(x_k))\n",
    "            history['grad_norm'].append(np.linalg.norm(grad_at_x_k))\n",
    "            if x_k.shape[0] <= 2:\n",
    "                history['x'].append(np.array(x_k))\n",
    "\n",
    "        if display:\n",
    "            print(\"iteration {}: |x_k-x_0| = {}, f(x_k) = {}, |grad(f(x_k))| = {}\".format(\n",
    "                i, np.linalg.norm(x_k - x_0), oracle.func(x_k), np.linalg.norm(grad_at_x_k))\n",
    "            )\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe4514",
   "metadata": {},
   "source": [
    "Now it's try to test gradient descent algorithm on different tasks. Here is some auxiliary code for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4153de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def get_color_array(size):\n",
    "    HSV_tuples = [(float(i) / size, 1.0, 1.0) for i in range(size)]\n",
    "    RGB_tuples = list(map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples))\n",
    "    return np.array(RGB_tuples)\n",
    "\n",
    "def plot_2d(history, oracle, x_0, x_opt, scale, title, n_levels=3, xlims=None, ylims=None, bound=None):\n",
    "    if bound is not None:\n",
    "        history['x'] = history['x'][:bound]\n",
    "    hist_x = np.array(history['x']).T\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.plot(np.linalg.norm(hist_x - x_opt[:, None], axis=0), c='c', linewidth=3)\n",
    "    plt.plot(history['grad_norm'], c='m', linewidth=2)\n",
    "    plt.xlim(0, len(history['grad_norm']) - 1)\n",
    "    plt.ylim(0, 2)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Gradients/residuals norm')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.legend(['Residuals', 'Gradients'])\n",
    "    plt.grid(ls=\":\")\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    if xlims is not None:\n",
    "        x_range = xlims\n",
    "    else:\n",
    "        x_range = [x_opt[0] - scale, x_opt[0] + scale]\n",
    "    if ylims is not None:\n",
    "        y_range = ylims\n",
    "    else:\n",
    "        y_range = [x_opt[1] - scale, x_opt[1] + scale]\n",
    "    func_opt = oracle.func(x_opt)\n",
    "    levels = [func_opt + 0.25 * scale / n_levels * i for i in range(1, 4*n_levels + 1, 4)]\n",
    "    # sizes = np.minimum(5 * float(hist_x.shape[1]) / (np.arange(1, hist_x.shape[1] + 1)), 20)\n",
    "    plt.scatter(hist_x[0], hist_x[1], s=8, c=get_color_array(hist_x.shape[1]))\n",
    "    plt.plot(hist_x[0], hist_x[1], alpha=0.6, ls=\"--\", c='r')\n",
    "    # plt.plot(hist_x[0], hist_x[1], c='r', linewidth=3)\n",
    "    plt.scatter(x_opt[0], x_opt[1], marker='s', c='r', edgecolors='k')\n",
    "    plot_levels(oracle.func, xrange=x_range, yrange=y_range, levels=levels)\n",
    "    plt.title('Trajectory')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(ls=\":\")\n",
    "    if xlims is not None:\n",
    "        plt.xlim(xlims)\n",
    "    if ylims is not None:\n",
    "        plt.ylim(ylims)\n",
    "    plt.show()\n",
    "\n",
    "def test_2d(method, oracle, x_0, lr, tol=1e-5, scale=1.0, xlims=None, ylims=None, bound=None):\n",
    "    x_opt = oracle.get_opt()\n",
    "    if method == 'gd':\n",
    "        x_star, msg, history = gradient_descent(\n",
    "            oracle, x_0, lr, tol, trace=True\n",
    "        )\n",
    "    elif method == 'newton':\n",
    "        x_star, msg, history = newton(\n",
    "            oracle, x_0, lr, tol, trace=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown error\")\n",
    "    print(msg, len(history['grad_norm']), \"iterations\")\n",
    "    plot_2d(history, oracle, x_0, x_opt, scale, 'lr = {}'.format(lr), xlims=xlims, ylims=ylims, bound=bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdf314",
   "metadata": {},
   "source": [
    "Take a simple quadratic function\n",
    "\n",
    "$$\n",
    "    f(x, y) = 2x^2 + 4xy + 5 y^2\n",
    "$$\n",
    "\n",
    "which ovbiously has minimum at $(0, 0)$. Now run gradient descent method with $\\eta = 0.01$, starting from point $(-2, -1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ea22cd",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m q_oracle \u001b[38;5;241m=\u001b[39m QuadraticOracle(A, b)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_oracle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 55\u001b[0m, in \u001b[0;36mtest_2d\u001b[1;34m(method, oracle, x_0, lr, tol, scale, xlims, ylims, bound)\u001b[0m\n\u001b[0;32m     53\u001b[0m x_opt \u001b[38;5;241m=\u001b[39m oracle\u001b[38;5;241m.\u001b[39mget_opt()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     x_star, msg, history \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     59\u001b[0m     x_star, msg, history \u001b[38;5;241m=\u001b[39m newton(\n\u001b[0;32m     60\u001b[0m         oracle, x_0, lr, tol, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(oracle, x_0, learning_rate, tolerance, max_iter, trace, display)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):     \n\u001b[0;32m     61\u001b[0m     grad_at_x_k \u001b[38;5;241m=\u001b[39m oracle\u001b[38;5;241m.\u001b[39mgrad(x_k)\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39mabs(x_k) \u001b[38;5;241m>\u001b[39m INF) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_at_x_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m INF):\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x_k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputational_error\u001b[39m\u001b[38;5;124m'\u001b[39m, history\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for abs(): 'NoneType'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 2], [2, 5]])\n",
    "b = np.zeros(2)\n",
    "q_oracle = QuadraticOracle(A, b)\n",
    "test_2d(\"gd\", q_oracle, np.array([-2.0, -1.0]), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fcab8a",
   "metadata": {},
   "source": [
    "If you did all correctly, gradient descent should converge here to optimal point. But this is not always the case:\n",
    "\n",
    "* if $\\eta$ is very small, convergence could be too slow\n",
    "* if $\\eta$ is very big, than gradient descent often diverges\n",
    "\n",
    "### Task 2.5 (1 point)\n",
    "\n",
    "Take previous quadratic oracle and try different learning rates and starting points. Find the smallest and the largest learning rates for which gradient descent is still convergent. Provide some characteristic plots. Also, plot a graph of required iterations versus $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de464d0",
   "metadata": {},
   "source": [
    "### GD for linear regression\n",
    "\n",
    "Let's try it for simple linear regression. Here is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d7391",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, -2.1], [1, 1.2], [1, 4.8]])\n",
    "b = np.array([-1, 2, 5])\n",
    "oracle = LinRegOracle(A, b)\n",
    "test_2d(\"gd\", oracle, np.array([-1, 3.0]), lr=0.01, tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a1c6f",
   "metadata": {},
   "source": [
    "### Task 2.6 (0.5 points)\n",
    "\n",
    "Apply gradient descent algorithm to simple linear regression trained on `boston` dataset. Take `lstat` as feature, `medv` as target. Provide some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c6dde",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston = pd.read_csv(\"../datasets/ISLP/Boston.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf78dd",
   "metadata": {},
   "source": [
    "### GD for logistic regression\n",
    "\n",
    "The targets must be from $\\{-1, 1\\}$. One more toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1cddf",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, -2.1], [1, 1.2], [1, 4.8], [1, 3.4]])\n",
    "b = np.array([-1, 1, 1, 1])\n",
    "oracle = LogRegOracle(A, b)\n",
    "test_2d(\"gd\", oracle, np.array([1., 2.0]), lr=0.01, tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c99f5",
   "metadata": {},
   "source": [
    "### Task 2.7 (0.5 points)\n",
    "\n",
    "Train simple logistic regression model on `breast_cancer` dataset using your gradient descent method. Take any feature from the dataset. Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa786d0",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast = load_breast_cancer()\n",
    "y = breast['target']\n",
    "y[y == 0] = -1\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7d396b",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "Given a starting point $\\boldsymbol x_0 \\in\\mathbb R^n$ and learning rate $\\eta > 0$, iteratively calculate\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{k+1} = \\boldsymbol x_k - \\eta \\big(\\nabla^2 f(\\boldsymbol x_k)\\big)^{-1} \\nabla f(\\boldsymbol x_k), \\quad k = 0, 1,\\ldots\n",
    "$$\n",
    "\n",
    "The process should stop when either $k = \\mathrm{max\\_iter}$ or \n",
    "\n",
    "$$\n",
    "\\Vert \\nabla f(\\boldsymbol x_k)\\Vert_2^2 \\leqslant \\varepsilon \\Vert \\nabla f(\\boldsymbol x_0)\\Vert_2^2\n",
    "$$\n",
    "\n",
    "where $\\varepsilon > 0$ is tolerance.\n",
    "\n",
    "### Task 2.8 (1 point)\n",
    "\n",
    "Complete the implementation of Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def newton(oracle, x_0, learning_rate, tolerance=1e-5, max_iter=100, trace=False, display=False):\n",
    "    \"\"\"\n",
    "    Newton's optimization method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oracle : BaseSmoothOracle-descendant object\n",
    "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
    "        function value, its gradient and Hessian respectively. If the Hessian\n",
    "        returned by the oracle is not positive-definite method stops with message=\"newton_direction_error\"\n",
    "    x_0 : np.array\n",
    "        Starting point for optimization algorithm\n",
    "    learning_rate : float\n",
    "        Learning rate.\n",
    "    tolerance : float\n",
    "        Epsilon value for stopping criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    trace : bool\n",
    "        If True, the progress information is appended into history dictionary during training.\n",
    "        Otherwise None is returned instead of history.\n",
    "    display : bool\n",
    "        If True, debug information is displayed during optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_star : np.array\n",
    "        The point found by the optimization procedure\n",
    "    message : string\n",
    "        'success' or the description of error:\n",
    "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
    "                the stopping criterion.\n",
    "            - 'newton_direction_error': in case of failure of solving linear system with Hessian matrix (e.g. non-invertible matrix).\n",
    "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
    "    history : dictionary of lists or None\n",
    "        Dictionary containing the progress information or None if trace=False.\n",
    "        Dictionary has to be organized as follows:\n",
    "            - history['time'] : list of floats, containing time passed from the start of the method\n",
    "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
    "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
    "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
    "    >> x_opt, message, history = newton(oracle, np.zeros(5), line_search_options={'method': 'Constant', 'c': 1.0})\n",
    "    >> print('Found optimal point: {}'.format(x_opt))\n",
    "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
    "    \"\"\"\n",
    "    history = defaultdict(list) if trace else None\n",
    "    x_k = np.copy(x_0)\n",
    "    grad_at_x_0 = oracle.grad(x_0)\n",
    "\n",
    "    INF = 1e100\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for i in range(max_iter + 1): \n",
    "        grad_at_x_k = oracle.grad(x_k)\n",
    "        hess_at_x_k = oracle.hess(x_k)  \n",
    "\n",
    "        if np.any(np.abs(x_k) > INF) or np.any(np.abs(grad_at_x_k) > INF):\n",
    "            return x_k, 'computational_error', history\n",
    "\n",
    "        if trace:\n",
    "            history['time'].append((datetime.now() - start_time).seconds)\n",
    "            history['func'].append(oracle.func(x_k))\n",
    "            history['grad_norm'].append(np.linalg.norm(grad_at_x_k))\n",
    "            if x_k.shape[0] <= 2:\n",
    "                history['x'].append(np.array(x_k))\n",
    "\n",
    "        if display:\n",
    "            print(\"iteration {}: |x_k-x_0| = {}, f(x_k) = {}, |grad(f(x_k))| = {}\".format(\n",
    "                i, np.linalg.norm(x_k - x_0), oracle.func(x_k), np.linalg.norm(grad_at_x_k))\n",
    "            )\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ae164",
   "metadata": {},
   "source": [
    "### Task 2.9 (1 point)\n",
    "\n",
    "Apply Newton's methods to the previous tasks:\n",
    "\n",
    "* quadratic function\n",
    "* simple linear regression trained on `boston` dataset\n",
    "* simple logistic regression trained on `breast_cancer` dataset\n",
    "\n",
    "Compare the number of iterations and time execution of GD and Newton methods in this examples. Visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884c5bf",
   "metadata": {},
   "source": [
    "## GD vs Newton\n",
    "\n",
    "### Task 2.10 (1 point)\n",
    "\n",
    "Run both gradient descent and Newton's methods on **full** `boston` and `breast_cancer` datasets. Do they converge? How many iterations are required for convergence? How much time? Justify your answers by numeric experiments and visualizations."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
