{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec1760a-f557-4124-ae8b-f807bbbfae3d",
   "metadata": {},
   "source": [
    "Task 2.1: Quadratic Oracle Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f09d8d-95ad-4e78-9a92-e3ceb3601cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QuadraticOracle:\n",
    "    def __init__(self, A, b, c):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def func(self, x):\n",
    "        \"\"\"Calculate the value of the quadratic function f(x).\"\"\"\n",
    "        return 0.5 * np.dot(x.T, np.dot(self.A, x)) - np.dot(self.b.T, x) + self.c\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"Calculate the gradient of the quadratic function, ∇f(x).\"\"\"\n",
    "        return np.dot(self.A, x) - self.b\n",
    "\n",
    "    def hess(self):\n",
    "        \"\"\"Return the Hessian matrix, which is constant for quadratic functions.\"\"\"\n",
    "        return self.A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7a23-31c4-481d-8768-a85e7ef49d50",
   "metadata": {},
   "source": [
    "Task 2.2: Linear Regression Oracle Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06f430-d921-446a-96e4-c75bd5701e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionOracle:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def func(self, w):\n",
    "        \"\"\"Calculate the value of the linear regression function f(w).\"\"\"\n",
    "        residuals = np.dot(self.X, w) - self.y\n",
    "        return 0.5 * np.dot(residuals.T, residuals)\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\"Calculate the gradient of the linear regression function, ∇f(w).\"\"\"\n",
    "        residuals = np.dot(self.X, w) - self.y\n",
    "        return np.dot(self.X.T, residuals)\n",
    "\n",
    "    def hess(self):\n",
    "        \"\"\"Return the Hessian matrix, which is constant for linear regression.\"\"\"\n",
    "        return np.dot(self.X.T, self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c73e7-2861-4f54-97f9-e6b66e8625da",
   "metadata": {},
   "source": [
    "Task 2.3: Logistic Regression Oracle Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f636fd9-86f9-4089-8e63-38e0f4fb24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionOracle:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def func(self, w):\n",
    "        \"\"\"Calculate the value of the logistic regression function f(w).\"\"\"\n",
    "        logits = np.dot(self.X, w)\n",
    "        return np.sum(np.log(1 + np.exp(-self.y * logits)))\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\"Calculate the gradient of the logistic regression function, ∇f(w).\"\"\"\n",
    "        logits = np.dot(self.X, w)\n",
    "        probabilities = 1 / (1 + np.exp(-self.y * logits))\n",
    "        errors = probabilities - 1\n",
    "        return np.dot(self.X.T, self.y * errors)\n",
    "\n",
    "    def hess(self, w):\n",
    "        \"\"\"Calculate the Hessian of the logistic regression function.\"\"\"\n",
    "        logits = np.dot(self.X, w)\n",
    "        probabilities = 1 / (1 + np.exp(-self.y * logits))\n",
    "        D = np.diag(probabilities * (1 - probabilities))\n",
    "        return np.dot(self.X.T, np.dot(D, self.X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0bf904-32bf-456c-834c-13a14012f682",
   "metadata": {},
   "source": [
    "Task 2.4: Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415230d5-4e01-428e-9dc6-a5ef035bc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, oracle, learning_rate=0.01, tolerance=1e-6, max_iter=1000):\n",
    "        self.oracle = oracle\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def optimize(self, w_init):\n",
    "        w = w_init\n",
    "        for i in range(self.max_iter):\n",
    "            gradient = self.oracle.grad(w)\n",
    "            if np.linalg.norm(gradient) < self.tolerance:\n",
    "                print(f\"Converged in {i} iterations.\")\n",
    "                break\n",
    "            w = w - self.learning_rate * gradient\n",
    "        return w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
